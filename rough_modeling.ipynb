{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning) # seaborn warning about not using data=... notation\n",
    "import seaborn as sns\n",
    "\n",
    "import os\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "\n",
    "sns.set(rc = {'figure.figsize':(25, 12)})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadRawData():\n",
    "    # Loading each csv into the list and concat them into one dataframe in one step \n",
    "    df = []\n",
    "\n",
    "    for file in os.listdir('data'):\n",
    "        temp = pd.read_csv(\n",
    "            f'data/{file}', \n",
    "            parse_dates = {'date': ['year', 'month', 'day', 'hour']}, \n",
    "            date_parser = lambda x: datetime.strptime(x, '%Y %m %d %H'),\n",
    "            keep_date_col = True # will be used as dummies\n",
    "        )\n",
    "\n",
    "        # Values for different stations in each city are simmilar, so we can take the mean of them \n",
    "        targetCols = [col for col in temp.columns if 'PM' in col]\n",
    "        temp['meanPM'] = temp[targetCols].mean(axis=1).round(2)\n",
    "\n",
    "        targetCols.extend(('No', 'Iprec'))\n",
    "        temp.drop(targetCols, axis=1, inplace=True)\n",
    "\n",
    "        # Adding the source of the data from the filename\n",
    "        temp['source'] = file.split('PM')[0]\n",
    "        df.append(temp)\n",
    "\n",
    "    df = pd.concat(df, axis = 0)\n",
    "\n",
    "    # Moving important columns to the front, will be usefull when categorical columns are converted to dummies\n",
    "    colsToMove = ['date', 'source', 'meanPM']\n",
    "    df = df[colsToMove + [col for col in df.columns if col not in colsToMove]]\n",
    "    df['dayOfWeek'] = df['date'].dt.dayofweek\n",
    "\n",
    "    df = df[df.date > datetime(2012, 1, 1)]\n",
    "\n",
    "    return df.reset_index(drop = True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data wrangling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 175315 entries, 0 to 175314\n",
      "Data columns (total 16 columns):\n",
      " #   Column         Non-Null Count   Dtype         \n",
      "---  ------         --------------   -----         \n",
      " 0   date           175315 non-null  datetime64[ns]\n",
      " 1   source         175315 non-null  object        \n",
      " 2   meanPM         158144 non-null  float64       \n",
      " 3   year           175315 non-null  object        \n",
      " 4   month          175315 non-null  object        \n",
      " 5   day            175315 non-null  object        \n",
      " 6   hour           175315 non-null  object        \n",
      " 7   season         175314 non-null  float64       \n",
      " 8   DEWP           174795 non-null  float64       \n",
      " 9   HUMI           174465 non-null  float64       \n",
      " 10  PRES           174452 non-null  float64       \n",
      " 11  TEMP           174797 non-null  float64       \n",
      " 12  cbwd           174802 non-null  object        \n",
      " 13  Iws            174790 non-null  float64       \n",
      " 14  precipitation  167445 non-null  float64       \n",
      " 15  dayOfWeek      175315 non-null  int64         \n",
      "dtypes: datetime64[ns](1), float64(8), int64(1), object(6)\n",
      "memory usage: 21.4+ MB\n"
     ]
    }
   ],
   "source": [
    "loadRawData().info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepareTrainTestSet():\n",
    "    df = loadRawData()\n",
    "\n",
    "    ### Replace incorrect values with NaN ###\n",
    "    df.DEWP          = df.DEWP.replace(-9999, np.nan)\n",
    "    df.DEWP          = df.DEWP.replace(-97, np.nan)\n",
    "\n",
    "    df.HUMI          = df.HUMI.replace(-9999, np.nan)\n",
    "    df.precipitation = df.precipitation.replace(999990, np.nan)\n",
    "\n",
    "\n",
    "    ### Fill missing values in independent variables ###\n",
    "    colsToFill = df.columns.to_list()\n",
    "    colsToFill.remove('meanPM')\n",
    "\n",
    "    # Missing values in the independent variables are rare, so they are just filled with the previous value\n",
    "    df[colsToFill] = df[colsToFill].fillna(method = 'ffill').fillna(method = 'bfill')\n",
    "\n",
    "\n",
    "    ### Lagging the variables ###\n",
    "    independentCols = ['DEWP', 'HUMI', 'PRES', 'TEMP', 'Iws', 'precipitation']\n",
    "    df[independentCols] = df[independentCols].shift(24) # 24 hours lag\n",
    "\n",
    "    # TODO fix cause grops\n",
    "    df['meanPM_24h']  = df['meanPM'].shift(24)\n",
    "    df['meanPM_7d']   = df['meanPM'].shift(24 * 7)\n",
    "    df['meanPM_30d']  = df['meanPM'].shift(24 * 30)\n",
    "    df['meanPM_365d'] = df['meanPM'].shift(24 * 365)\n",
    "\n",
    "\n",
    "    ### Convert categorical to dummies ###\n",
    "    catCols = ['source', 'month', 'day', 'hour', 'season', 'cbwd', 'dayOfWeek']\n",
    "    temp = [df.drop(catCols, axis = 1)]\n",
    "    temp.extend(pd.get_dummies(df[col], prefix = col) for col in catCols)\n",
    "    df = pd.concat(temp, axis = 1)\n",
    "    \n",
    "    \n",
    "    ### Designate last year (~20%) of the data as test set ###\n",
    "    df['isTestSet'] = (df.date > datetime(2015, 1, 1)).astype(int)\n",
    "    \n",
    "    \n",
    "    ### Target col for classification ###\n",
    "    df['isDangerous'] = (df.meanPM > 100).astype(int)\n",
    "    \n",
    "    \n",
    "    ### Scale the numerical columns ###\n",
    "    numCols = ['meanPM', 'DEWP', 'HUMI', 'PRES', 'TEMP', 'Iws', 'precipitation', 'meanPM_24h', 'meanPM_7d', 'meanPM_30d', 'meanPM_365d']\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(df[df.isTestSet == 0][numCols])\n",
    "    \n",
    "    df[numCols] = scaler.transform(df[numCols])\n",
    "    \n",
    "    \n",
    "    ### Drop rows with NaN values ###\n",
    "    # 25% of the dataset is dropped. This is just a quick analysis so it's ok\n",
    "    # In a production model, the missing values should be investigated and filled with more sophisticated methods\n",
    "    # e.g. using a moving average (but the gaps are wider than 24 hours, so it's not ideal)\n",
    "    df = df[~df.isnull().any(axis = 1)].reset_index(drop = True)\n",
    "\n",
    "\n",
    "    ### Drop unnecessary columns ###\n",
    "    df = df.drop(['date', 'year'], axis = 1)\n",
    "\n",
    "\n",
    "    ### Split the data into train and test sets ###\n",
    "    independentCols = [col for col in df.columns if col not in ['meanPM', 'isTestSet', 'isDangerous']]\n",
    "    \n",
    "    X_train = df[df.isTestSet == 0][independentCols]\n",
    "    X_test  = df[df.isTestSet == 1][independentCols]\n",
    "    \n",
    "    y_train = df[df.isTestSet == 0]['meanPM']\n",
    "    y_test  = df[df.isTestSet == 1]['meanPM']\n",
    "    \n",
    "    y_train_class = df[df.isTestSet == 0]['isDangerous']\n",
    "    y_test_class  = df[df.isTestSet == 1]['isDangerous']\n",
    "\n",
    "    return X_train, X_test, y_train, y_test, y_train_class, y_test_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train, X_test, y_train, y_test, y_train_class, y_test_class = prepareTrainTestSet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    #from sklearn.pipeline import Pipeline\n",
    "    ##from sklearn.compose import ColumnTransformer\n",
    "    #from sklearn.preprocessing import OneHotEncoder\n",
    "    \n",
    "    # sklearn pipeline did not work well\n",
    "    \n",
    "    #numCols = ['DEWP', 'HUMI', 'PRES', 'TEMP', 'Iws', 'precipitation', 'meanPM_24h', 'meanPM_7d', 'meanPM_30d', 'meanPM_365d']\n",
    "    #catCols = ['source', 'month', 'day', 'hour', 'season', 'cbwd', 'dayOfWeek']\n",
    "    #dropCols  = ['date', 'year']\n",
    "    #passCols  = ['meanPM']\n",
    "    #\n",
    "    #\n",
    "    #fullPipeline = ColumnTransformer([\n",
    "    #    ('target', 'passthrough', passCols),\n",
    "    #    ('num', StandardScaler(), numCols),\n",
    "    #    ('cat', OneHotEncoder(), catCols),\n",
    "    #    ('drop', 'drop', dropCols)\n",
    "    #]) \n",
    "    #\n",
    "    #df = pd.DataFrame(fullPipeline.fit_transform(df).todense())\n",
    "    #\n",
    "    ##df.columns = passCols + numCols + fullPipeline.named_transformers_['cat'].get_feature_names(catCols)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test, y_train_class, y_test_class = prepareTrainTestSet()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8356830112555304"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classDummy = DummyClassifier(strategy = 'most_frequent')\n",
    "classDummy.fit(X_train, y_train_class)\n",
    "classDummy.score(X_test, y_test_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Done 168 tasks      | elapsed:    1.0s\n",
      "[Parallel(n_jobs=-1)]: Done 418 tasks      | elapsed:    2.5s\n",
      "[Parallel(n_jobs=-1)]: Done 768 tasks      | elapsed:    4.5s\n",
      "[Parallel(n_jobs=-1)]: Done 1000 out of 1000 | elapsed:    5.8s finished\n",
      "[Parallel(n_jobs=16)]: Using backend ThreadingBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=16)]: Done  18 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=16)]: Done 168 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=16)]: Done 418 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=16)]: Done 768 tasks      | elapsed:    0.3s\n",
      "[Parallel(n_jobs=16)]: Done 1000 out of 1000 | elapsed:    0.4s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8627329619696031"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classRF = RandomForestClassifier(n_estimators = 1000, max_depth = 10, random_state = 42, verbose = 1, n_jobs = -1)\n",
    "classRF.fit(X_train, y_train_class)\n",
    "classRF.score(X_test, y_test_class)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:    1.4s\n",
      "[Parallel(n_jobs=-1)]: Done 168 tasks      | elapsed:    9.6s\n",
      "[Parallel(n_jobs=-1)]: Done 418 tasks      | elapsed:   22.9s\n",
      "[Parallel(n_jobs=-1)]: Done 768 tasks      | elapsed:   41.0s\n",
      "[Parallel(n_jobs=-1)]: Done 1000 out of 1000 | elapsed:   53.5s finished\n",
      "[Parallel(n_jobs=16)]: Using backend ThreadingBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=16)]: Done  18 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=16)]: Done 168 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=16)]: Done 418 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=16)]: Done 768 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=16)]: Done 1000 out of 1000 | elapsed:    0.2s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.3636056083039695"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regRF = RandomForestRegressor(n_estimators = 1000, max_depth = 10, random_state = 42, verbose = 1, n_jobs = -1)\n",
    "regRF.fit(X_train, y_train)\n",
    "regRF.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "meanPM_24h         44.046676\n",
       "DEWP                7.507327\n",
       "PRES                5.127452\n",
       "meanPM_30d          4.141416\n",
       "cbwd_cv             3.279316\n",
       "meanPM_7d           2.919822\n",
       "meanPM_365d         2.806952\n",
       "cbwd_NW             2.286696\n",
       "source_Beijing      2.209434\n",
       "day_31              1.705611\n",
       "source_Shanghai     1.599731\n",
       "TEMP                1.561469\n",
       "Iws                 1.544804\n",
       "HUMI                1.469516\n",
       "month_1             1.374907\n",
       "cbwd_NE             1.250040\n",
       "cbwd_SE             1.166417\n",
       "day_12              1.123863\n",
       "source_Shenyang     0.900455\n",
       "day_6               0.722017\n",
       "dtype: float64"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# feature importance as a table with col names\n",
    "pd.Series(regRF.feature_importances_, index = X_train.columns).sort_values(ascending = False).head(20) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "36ab7f9348c7d0662a1965af503f4376fbff05df74ea3a3ead8d3abcf8650cb8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
