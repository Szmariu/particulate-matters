{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning) # seaborn warning about not using data=... notation\n",
    "import seaborn as sns\n",
    "\n",
    "import os\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "sns.set(rc = {'figure.figsize':(25, 12)})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadRawData():\n",
    "    # Loading each csv into the list and concat them into one dataframe in one step \n",
    "    df = []\n",
    "\n",
    "    for file in os.listdir('data'):\n",
    "        temp = pd.read_csv(\n",
    "            f'data/{file}', \n",
    "            parse_dates = {'date': ['year', 'month', 'day', 'hour']}, \n",
    "            date_parser = lambda x: datetime.strptime(x, '%Y %m %d %H'),\n",
    "            keep_date_col = True # will be used as dummies\n",
    "        )\n",
    "\n",
    "        # Values for different stations in each city are simmilar, so we can take the mean of them \n",
    "        targetCols = [col for col in temp.columns if 'PM' in col]\n",
    "        temp['meanPM'] = temp[targetCols].mean(axis=1).round(2)\n",
    "\n",
    "        targetCols.extend(('No', 'Iprec'))\n",
    "        temp.drop(targetCols, axis=1, inplace=True)\n",
    "\n",
    "        # Adding the source of the data from the filename\n",
    "        temp['source'] = file.split('PM')[0]\n",
    "        df.append(temp)\n",
    "\n",
    "    df = pd.concat(df, axis = 0)\n",
    "\n",
    "    # Moving important columns to the front, will be usefull when categorical columns are converted to dummies\n",
    "    colsToMove = ['date', 'source', 'meanPM']\n",
    "    df = df[colsToMove + [col for col in df.columns if col not in colsToMove]]\n",
    "    df['dayOfWeek'] = df['date'].dt.dayofweek\n",
    "\n",
    "    df = df[df.date > datetime(2012, 1, 1)]\n",
    "\n",
    "    return df.reset_index(drop = True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data wrangling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 175315 entries, 0 to 175314\n",
      "Data columns (total 16 columns):\n",
      " #   Column         Non-Null Count   Dtype         \n",
      "---  ------         --------------   -----         \n",
      " 0   date           175315 non-null  datetime64[ns]\n",
      " 1   source         175315 non-null  object        \n",
      " 2   meanPM         158144 non-null  float64       \n",
      " 3   year           175315 non-null  object        \n",
      " 4   month          175315 non-null  object        \n",
      " 5   day            175315 non-null  object        \n",
      " 6   hour           175315 non-null  object        \n",
      " 7   season         175314 non-null  float64       \n",
      " 8   DEWP           174795 non-null  float64       \n",
      " 9   HUMI           174465 non-null  float64       \n",
      " 10  PRES           174452 non-null  float64       \n",
      " 11  TEMP           174797 non-null  float64       \n",
      " 12  cbwd           174802 non-null  object        \n",
      " 13  Iws            174790 non-null  float64       \n",
      " 14  precipitation  167445 non-null  float64       \n",
      " 15  dayOfWeek      175315 non-null  int64         \n",
      "dtypes: datetime64[ns](1), float64(8), int64(1), object(6)\n",
      "memory usage: 21.4+ MB\n"
     ]
    }
   ],
   "source": [
    "loadRawData().info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepareTrainTestSet():\n",
    "    df = loadRawData()\n",
    "\n",
    "    ### Replace incorrect values with NaN ###\n",
    "    df.DEWP          = df.DEWP.replace(-9999, np.nan)\n",
    "    df.DEWP          = df.DEWP.replace(-97, np.nan)\n",
    "\n",
    "    df.HUMI          = df.HUMI.replace(-9999, np.nan)\n",
    "    df.precipitation = df.precipitation.replace(999990, np.nan)\n",
    "\n",
    "\n",
    "    ### Fill missing values in independent variables ###\n",
    "    colsToFill = df.columns.to_list()\n",
    "    colsToFill.remove('meanPM')\n",
    "\n",
    "    # Missing values in the independent variables are rare, so they are just filled with the previous value\n",
    "    df[colsToFill] = df[colsToFill].fillna(method = 'ffill').fillna(method = 'bfill')\n",
    "\n",
    "\n",
    "    ### Lagging the variables ###\n",
    "    independentCols = ['DEWP', 'HUMI', 'PRES', 'TEMP', 'Iws', 'precipitation']\n",
    "    df[independentCols] = df[independentCols].shift(24) # 24 hours lag\n",
    "\n",
    "    # TODO fix cause grops\n",
    "    df['meanPM_24h']  = df['meanPM'].shift(24)\n",
    "    df['meanPM_7d']   = df['meanPM'].shift(24 * 7)\n",
    "    df['meanPM_30d']  = df['meanPM'].shift(24 * 30)\n",
    "    df['meanPM_365d'] = df['meanPM'].shift(24 * 365)\n",
    "\n",
    "\n",
    "    ### Convert categorical to dummies ###\n",
    "    catCols = ['source', 'month', 'day', 'hour', 'season', 'cbwd', 'dayOfWeek']\n",
    "    temp = [df.drop(catCols, axis = 1)]\n",
    "    temp.extend(pd.get_dummies(df[col], prefix = col) for col in catCols)\n",
    "    df = pd.concat(temp, axis = 1)\n",
    "    \n",
    "    \n",
    "    ### Designate last year (~20%) of the data as test set ###\n",
    "    df['isTestSet'] = (df.date > datetime(2015, 1, 1)).astype(int)\n",
    "    \n",
    "    \n",
    "    ### Target col for classification ###\n",
    "    df['isDangerous'] = (df.meanPM > 150).astype(int)\n",
    "    \n",
    "    \n",
    "    ### Scale the numerical columns ###\n",
    "    numCols = ['meanPM', 'DEWP', 'HUMI', 'PRES', 'TEMP', 'Iws', 'precipitation', 'meanPM_24h', 'meanPM_7d', 'meanPM_30d', 'meanPM_365d']\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(df[df.isTestSet == 0][numCols])\n",
    "    \n",
    "    df[numCols] = scaler.transform(df[numCols])\n",
    "    \n",
    "    \n",
    "    ### Drop rows with NaN values ###\n",
    "    # 25% of the dataset is dropped. This is just a quick analysis so it's ok\n",
    "    # In a production model, the missing values should be investigated and filled with more sophisticated methods\n",
    "    df = df[~df.isnull().any(axis = 1)].reset_index(drop = True)\n",
    "\n",
    "\n",
    "    ### Drop unnecessary columns ###\n",
    "    df = df.drop(['date', 'year'], axis = 1)\n",
    "\n",
    "    independentCols = [col for col in df.columns if col not in ['meanPM', 'isTestSet', 'isDangerous']]\n",
    "    \n",
    "    X_train = df[df.isTestSet == 0][independentCols]\n",
    "    X_test  = df[df.isTestSet == 1][independentCols]\n",
    "    \n",
    "    y_train = df[df.isTestSet == 0]['meanPM']\n",
    "    y_test  = df[df.isTestSet == 1]['meanPM']\n",
    "    \n",
    "    y_train_class = df[df.isTestSet == 0]['isDangerous']\n",
    "    y_test_class  = df[df.isTestSet == 1]['isDangerous']\n",
    "\n",
    "    return X_train, X_test, y_train, y_test, y_train_class, y_test_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test, y_train_class, y_test_class = prepareTrainTestSet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # sklearn pipeline did not work well\n",
    "    \n",
    "    #numCols = ['DEWP', 'HUMI', 'PRES', 'TEMP', 'Iws', 'precipitation', 'meanPM_24h', 'meanPM_7d', 'meanPM_30d', 'meanPM_365d']\n",
    "    #catCols = ['source', 'month', 'day', 'hour', 'season', 'cbwd', 'dayOfWeek']\n",
    "    #dropCols  = ['date', 'year']\n",
    "    #passCols  = ['meanPM']\n",
    "    #\n",
    "    #\n",
    "    #fullPipeline = ColumnTransformer([\n",
    "    #    ('target', 'passthrough', passCols),\n",
    "    #    ('num', StandardScaler(), numCols),\n",
    "    #    ('cat', OneHotEncoder(), catCols),\n",
    "    #    ('drop', 'drop', dropCols)\n",
    "    #]) \n",
    "    #\n",
    "    #df = pd.DataFrame(fullPipeline.fit_transform(df).todense())\n",
    "    #\n",
    "    ##df.columns = passCols + numCols + fullPipeline.named_transformers_['cat'].get_feature_names(catCols)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "36ab7f9348c7d0662a1965af503f4376fbff05df74ea3a3ead8d3abcf8650cb8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
